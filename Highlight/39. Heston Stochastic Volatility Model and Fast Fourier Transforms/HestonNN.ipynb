{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:48.227468Z",
     "start_time": "2025-11-11T21:29:48.226164Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:48.302951Z",
     "start_time": "2025-11-11T21:29:48.230798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# =========================\n",
    "# Heston + Carr–Madan (FFT)\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class HestonParams:\n",
    "    kappa: float\n",
    "    theta: float\n",
    "    sigma: float\n",
    "    rho: float\n",
    "    v0: float\n",
    "\n",
    "def heston_cf(u, T, S0, r, q, p: HestonParams):\n",
    "    \"\"\"\n",
    "    Characteristic function φ(u) = E[exp(i u ln S_T)] under Q.\n",
    "    Uses the Little Heston Trap parametrization.\n",
    "    u can be scalar or numpy array.\n",
    "    \"\"\"\n",
    "    i = 1j\n",
    "    x0 = np.log(S0)\n",
    "    a  = p.kappa * p.theta\n",
    "    b  = p.kappa - p.rho * p.sigma * i * u\n",
    "    d  = np.sqrt(b*b + (p.sigma**2) * (i*u + u*u))\n",
    "    g  = (b - d) / (b + d)\n",
    "\n",
    "    eDT = np.exp(-d * T)\n",
    "    one_minus_g_eDT = 1 - g * eDT\n",
    "    one_minus_g     = 1 - g\n",
    "    # small guards\n",
    "    one_minus_g_eDT = np.where(np.abs(one_minus_g_eDT) < 1e-15, 1e-15, one_minus_g_eDT)\n",
    "    one_minus_g     = np.where(np.abs(one_minus_g)     < 1e-15, 1e-15, one_minus_g)\n",
    "\n",
    "    C = i*u*(r - q)*T + (a/(p.sigma**2)) * ((b - d)*T - 2.0*np.log(one_minus_g_eDT/one_minus_g))\n",
    "    D = ((b - d)/(p.sigma**2)) * ((1 - eDT) / one_minus_g_eDT)\n",
    "    return np.exp(C + D*p.v0 + i*u*x0)\n",
    "\n",
    "def _simpson_weights(N: int):\n",
    "    \"\"\"Simpson weights on an N-point uniform grid (N must be even).\"\"\"\n",
    "    if N % 2 != 0:\n",
    "        raise ValueError(\"N must be even for Simpson weights.\")\n",
    "    w = np.ones(N)\n",
    "    w[1:N-1:2] = 4\n",
    "    w[2:N-2:2] = 2\n",
    "    return w\n",
    "\n",
    "def heston_fft_calls(\n",
    "    S0: float,\n",
    "    T: float,\n",
    "    r: float,\n",
    "    q: float,\n",
    "    p: HestonParams,\n",
    "    N: int = 4096,      # power-of-two recommended; must be even\n",
    "    eta: float = 0.25,  # frequency step Δv\n",
    "    alpha: float = 1.5  # damping (>0)\n",
    "):\n",
    "    \"\"\"\n",
    "    Carr–Madan FFT for call prices across a K-grid.\n",
    "    IMPORTANT: k = ln K (log-STRIKE), not ln(K/S0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : (N,) ascending strikes\n",
    "    C : (N,) call prices for these K\n",
    "    \"\"\"\n",
    "    n = np.arange(N)\n",
    "    v = eta * n  # frequency grid\n",
    "\n",
    "    i = 1j\n",
    "    # ψ(v) = e^{-rT} φ(v - i(α+1)) / [(α + iv)(α + iv + 1)]\n",
    "    phi_shift = heston_cf(v - (alpha + 1)*i, T, S0, r, q, p)\n",
    "    denom = (alpha**2 + alpha - v**2 + i*(2*alpha + 1)*v)  # (α+iv)(α+iv+1)\n",
    "    psi = np.exp(-r*T) * phi_shift / denom\n",
    "\n",
    "    # Simpson weights for the v-integral\n",
    "    w = _simpson_weights(N) * (eta / 3.0)\n",
    "\n",
    "    # FFT coupling\n",
    "    lam = 2.0 * np.pi / (N * eta)   # Δk (log-strike step)\n",
    "    b   = 0.5 * N * lam             # half-width in k\n",
    "    x   = psi * np.exp(1j * b * v) * w\n",
    "\n",
    "    F   = np.fft.fft(x)\n",
    "    F   = np.real(F)\n",
    "\n",
    "    j = np.arange(N)\n",
    "    k = -b + j * lam                 # k = ln K\n",
    "    K = np.exp(k)\n",
    "\n",
    "    calls = np.exp(-alpha * k) / np.pi * F\n",
    "    order = np.argsort(K)\n",
    "    return K[order], np.maximum(calls[order], 0.0)\n",
    "\n",
    "def heston_fft_call_price(\n",
    "    S0: float, K: float, T: float, r: float, q: float, p: HestonParams,\n",
    "    N: int = 4096, eta: float = 0.25, alpha: float = 1.5\n",
    "):\n",
    "    \"\"\"Price a single call via FFT + linear interpolation on the K-grid.\"\"\"\n",
    "    K_grid, C_grid = heston_fft_calls(S0, T, r, q, p, N=N, eta=eta, alpha=alpha)\n",
    "    if K <= K_grid[0]:\n",
    "        return C_grid[0]\n",
    "    if K >= K_grid[-1]:\n",
    "        return C_grid[-1]\n",
    "    idx = np.searchsorted(K_grid, K)\n",
    "    x0, x1 = K_grid[idx-1], K_grid[idx]\n",
    "    y0, y1 = C_grid[idx-1], C_grid[idx]\n",
    "    return y0 + (y1 - y0) * (K - x0) / (x1 - x0)\n",
    "\n",
    "def heston_fft_put_price(\n",
    "    S0: float, K: float, T: float, r: float, q: float, p: HestonParams,\n",
    "    N: int = 4096, eta: float = 0.25, alpha: float = 1.5\n",
    "):\n",
    "    \"\"\"Put via put–call parity.\"\"\"\n",
    "    C = heston_fft_call_price(S0, K, T, r, q, p, N=N, eta=eta, alpha=alpha)\n",
    "    return C - S0*np.exp(-q*T) + K*np.exp(-r*T)\n",
    "\n",
    "\n"
   ],
   "id": "13683d0e8e662baf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:48.423524Z",
     "start_time": "2025-11-11T21:29:48.414313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parameters\n",
    "S0, r, q, T = 100.0, 0.01, 0.00, 1.0\n",
    "hp = HestonParams(kappa=1.5, theta=0.04, sigma=0.3, rho=-0.7, v0=0.04)\n",
    "\n",
    "# full grid of calls\n",
    "K, C = heston_fft_calls(S0, T, r, q, hp, N=4096, eta=0.25, alpha=1.5)\n",
    "\n",
    "# single strikes\n",
    "call_100 = heston_fft_call_price(S0, 100, T, r, q, hp)\n",
    "\n",
    "call_100\n"
   ],
   "id": "e48038150101805f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.07774347291964)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:48.624786Z",
     "start_time": "2025-11-11T21:29:48.466963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simulate Heston price with given parameters\n",
    "S0 = 100          # Initial stock price\n",
    "K = 100           # Strike price\n",
    "T = 1             # Time to maturity\n",
    "r = 0.01          # Risk-free rate\n",
    "v0 = 0.04         # Initial variance\n",
    "kappa = 2         # Mean reversion speed\n",
    "theta = 0.04      # Long-run variance\n",
    "sigma = 0.3       # Volatility of variance\n",
    "rho = -0.7        # Correlation\n",
    "N = 16384         # Number of grid points\n",
    "alpha = 1.5       # Damping factor\n",
    "\n",
    "# Simulation parameters\n",
    "n_paths = 10000   # Number of paths\n",
    "n_steps = 252     # Number of time steps (daily)\n",
    "dt = T/n_steps    # Time step size\n",
    "\n",
    "# Initialize arrays for paths\n",
    "S = np.zeros((n_paths, n_steps + 1))\n",
    "v = np.zeros((n_paths, n_steps + 1))\n",
    "\n",
    "# Set initial values\n",
    "S[:, 0] = S0\n",
    "v[:, 0] = v0\n",
    "\n",
    "# Generate correlated random numbers\n",
    "rng = np.random.default_rng()\n",
    "dW1 = rng.normal(0, np.sqrt(dt), (n_paths, n_steps))\n",
    "dW2 = rho * dW1 + np.sqrt(1 - rho**2) * rng.normal(0, np.sqrt(dt), (n_paths, n_steps))\n",
    "\n",
    "# Euler-Maruyama simulation\n",
    "for t in range(n_steps):\n",
    "    # Ensure variance stays positive\n",
    "    v[:, t] = np.maximum(v[:, t], 0)\n",
    "\n",
    "    # Update stock price\n",
    "    S[:, t+1] = S[:, t] * np.exp((r - 0.5*v[:, t])*dt + np.sqrt(v[:, t])*dW1[:, t])\n",
    "\n",
    "    # Update variance\n",
    "    v[:, t+1] = v[:, t] + kappa*(theta - v[:, t])*dt + sigma*np.sqrt(v[:, t])*dW2[:, t]\n",
    "\n",
    "# Calculate call option prices\n",
    "payoffs = np.maximum(S[:, -1] - K, 0)\n",
    "discounted_payoffs = np.exp(-r*T) * payoffs\n",
    "price = np.mean(discounted_payoffs)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "std_error = np.std(discounted_payoffs) / np.sqrt(n_paths)\n",
    "ci_lower = price - 1.96 * std_error\n",
    "ci_upper = price + 1.96 * std_error\n",
    "\n",
    "print(f\"Heston call option price: {price:.8f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.8f}, {ci_upper:.8f}]\")\n",
    "\n"
   ],
   "id": "b4ce4a7d2232faf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heston call option price: 8.10766805\n",
      "95% Confidence Interval: [7.89253969, 8.32279640]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:48.640613Z",
     "start_time": "2025-11-11T21:29:48.635312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate strike grid centered around S0\n",
    "strikes = np.arange(S0-20, S0+25, 5)\n",
    "\n",
    "# Get FFT prices for all strikes\n",
    "K_fft, C_fft = heston_fft_calls(S0, T, r, q, hp, N=4096, eta=0.25, alpha=1.5)\n",
    "\n",
    "# Interpolate FFT prices to our strike grid\n",
    "fft_prices = np.interp(strikes, K_fft, C_fft)\n",
    "\n",
    "# Calculate MC prices for all strikes\n",
    "mc_prices = []\n",
    "mc_ci_lower = []\n",
    "mc_ci_upper = []\n",
    "\n",
    "for K in strikes:\n",
    "    payoffs = np.maximum(S[:, -1] - K, 0)\n",
    "    discounted_payoffs = np.exp(-r*T) * payoffs\n",
    "    price = np.mean(discounted_payoffs)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    std_error = np.std(discounted_payoffs) / np.sqrt(n_paths)\n",
    "    mc_prices.append(price)\n",
    "    mc_ci_lower.append(price - 1.96 * std_error)\n",
    "    mc_ci_upper.append(price + 1.96 * std_error)\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nPrice Comparison:\")\n",
    "print(\"Strike   FFT Price   MC Price   MC 95% CI\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(len(strikes)):\n",
    "    print(f\"{strikes[i]:6.1f}   {fft_prices[i]:9.4f}   {mc_prices[i]:8.4f}   [{mc_ci_lower[i]:8.4f}, {mc_ci_upper[i]:8.4f}]\")\n",
    "\n"
   ],
   "id": "37bba4e8549f0d1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price Comparison:\n",
      "Strike   FFT Price   MC Price   MC 95% CI\n",
      "---------------------------------------------\n",
      "  80.0     22.3804    22.3013   [ 21.9792,  22.6234]\n",
      "  85.0     18.2617    18.2023   [ 17.9013,  18.5033]\n",
      "  90.0     14.4595    14.4111   [ 14.1351,  14.6871]\n",
      "  95.0     11.0425    11.0176   [ 10.7704,  11.2648]\n",
      " 100.0      8.0777     8.1077   [  7.8925,   8.3228]\n",
      " 105.0      5.6161     5.7018   [  5.5203,   5.8833]\n",
      " 110.0      3.6817     3.7996   [  3.6514,   3.9479]\n",
      " 115.0      2.2629     2.4006   [  2.2837,   2.5175]\n",
      " 120.0      1.2993     1.4408   [  1.3519,   1.5296]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:29:51.086496Z",
     "start_time": "2025-11-11T21:29:48.686326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install torch --upgrade\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# Torch Heston + Carr–Madan\n",
    "# =========================\n",
    "\n",
    "class HestonPricingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable Heston pricer (Carr–Madan FFT) in torch.\n",
    "    Vectorized over batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, N=4096, eta=0.25, alpha=1.5):\n",
    "        super().__init__()\n",
    "        if N % 2 != 0:\n",
    "            raise ValueError(\"N must be even.\")\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"alpha must be > 0.\")\n",
    "        self.N = N\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Precompute frequency grid and Simpson weights as buffers\n",
    "        n = torch.arange(N, dtype=torch.float64)  # use float64 for stability\n",
    "        v = eta * n\n",
    "        w = torch.ones(N, dtype=torch.float64)\n",
    "        w[1:N-1:2] = 4.0\n",
    "        w[2:N-2:2] = 2.0\n",
    "        w = w * (eta / 3.0)\n",
    "\n",
    "        self.register_buffer(\"v\", v)\n",
    "        self.register_buffer(\"w\", w)\n",
    "\n",
    "        dk = 2.0 * torch.pi / (N * eta)\n",
    "        b = 0.5 * N * dk\n",
    "        self.dk = dk\n",
    "        self.b = b\n",
    "\n",
    "    def _heston_cf(self, u, T, S0, r, q, kappa, theta, sigma, rho, v0):\n",
    "        \"\"\"\n",
    "        Little Heston Trap CF in torch, vectorized over batch and frequency.\n",
    "        Inputs:\n",
    "            u: (..., N) complex\n",
    "            scalars or tensors broadcastable for T,S0,r,q,kappa,theta,sigma,rho,v0\n",
    "        \"\"\"\n",
    "        i = torch.complex(torch.tensor(0., dtype=torch.float64, device=u.device),\n",
    "                          torch.tensor(1., dtype=torch.float64, device=u.device))\n",
    "        x0 = torch.log(S0)\n",
    "\n",
    "        a = kappa * theta\n",
    "        b = kappa - rho * sigma * i * u\n",
    "        d = torch.sqrt(b*b + (sigma**2) * (u*u + i*u))\n",
    "\n",
    "        g = (b - d) / (b + d)\n",
    "        # trap: enforce |g|<1\n",
    "        mask = (g.abs() >= 1.0)\n",
    "        g = torch.where(mask, 1.0 / g, g)\n",
    "\n",
    "        eDT = torch.exp(-d * T)\n",
    "        one_minus_g = 1.0 - g\n",
    "        one_minus_g_eDT = 1.0 - g * eDT\n",
    "\n",
    "        eps = torch.tensor(1e-15, dtype=torch.float64, device=u.device)\n",
    "        one_minus_g = torch.where(one_minus_g.abs() < eps, eps, one_minus_g)\n",
    "        one_minus_g_eDT = torch.where(one_minus_g_eDT.abs() < eps, eps, one_minus_g_eDT)\n",
    "\n",
    "        C = i*u*(r - q)*T + (a/(sigma**2)) * ((b - d)*T - 2.0*torch.log(one_minus_g_eDT/one_minus_g))\n",
    "        D = ((b - d)/(sigma**2)) * ((1.0 - eDT)/one_minus_g_eDT)\n",
    "\n",
    "        return torch.exp(C + D*v0 + i*u*x0)\n",
    "\n",
    "    def forward(self, S0, K, T, r, q, kappa, theta, sigma, rho, v0):\n",
    "        \"\"\"\n",
    "        Returns call price C(S0,K,T,r,q; params)\n",
    "        All inputs are tensors of shape (batch,)\n",
    "        \"\"\"\n",
    "        device = S0.device\n",
    "        dtype = torch.float64\n",
    "\n",
    "        v = self.v.to(device=device)  # (N,)\n",
    "        w = self.w.to(device=device)  # (N,)\n",
    "        N = self.N\n",
    "        alpha = torch.as_tensor(self.alpha, dtype=dtype, device=device)\n",
    "\n",
    "        # Build shifted CF on the batch × N grid\n",
    "        # u = v - i(α+1)\n",
    "        i = torch.complex(torch.tensor(0., dtype=dtype, device=device),\n",
    "                          torch.tensor(1., dtype=dtype, device=device))\n",
    "        u_shift = v - (alpha + 1.0)*i  # (N,) complex\n",
    "\n",
    "        # Broadcast u over batch: (B, N)\n",
    "        B = S0.shape[0]\n",
    "        u = u_shift.unsqueeze(0).expand(B, N)\n",
    "\n",
    "        # Cast all reals to float64\n",
    "        S0 = S0.to(dtype)\n",
    "        K  = K.to(dtype)\n",
    "        T  = T.to(dtype)\n",
    "        r  = r.to(dtype)\n",
    "        q  = q.to(dtype)\n",
    "        kappa = kappa.to(dtype)\n",
    "        theta = theta.to(dtype)\n",
    "        sigma = sigma.to(dtype)\n",
    "        rho   = rho.to(dtype)\n",
    "        v0    = v0.to(dtype)\n",
    "\n",
    "        phi_shift = self._heston_cf(u, T[:,None], S0[:,None], r[:,None], q[:,None],\n",
    "                                    kappa[:,None], theta[:,None], sigma[:,None],\n",
    "                                    rho[:,None], v0[:,None])  # (B,N) complex\n",
    "\n",
    "        denom = (alpha**2 + alpha - v**2 + (2*alpha + 1.0)*i*v)  # (N,) complex\n",
    "        denom = torch.where(denom.abs() < 1e-30, torch.complex(torch.tensor(1e-30, dtype=dtype, device=device),\n",
    "                                                               torch.tensor(0., dtype=dtype, device=device)),\n",
    "                            denom)\n",
    "        psi = torch.exp(-r[:,None]*T[:,None]) * phi_shift / denom  # (B,N) complex\n",
    "\n",
    "        # FFT coupling\n",
    "        dk = self.dk.to(device=device)\n",
    "        b  = self.b.to(device=device)\n",
    "        x  = psi * torch.exp(1j * b * v) * w  # (B,N) complex\n",
    "\n",
    "        # torch.fft.fft over last dim\n",
    "        F = torch.fft.fft(x, dim=-1)  # (B,N) complex\n",
    "        F = F.real  # (B,N)\n",
    "\n",
    "        j = torch.arange(N, dtype=dtype, device=device)\n",
    "        k_grid = -b + j * dk      # (N,)\n",
    "        K_grid = torch.exp(k_grid)\n",
    "\n",
    "        calls_grid = torch.exp(-alpha * k_grid)[None, :] / torch.pi * F  # (B,N)\n",
    "\n",
    "        # Interp lineaire en log-strike\n",
    "        k_target = torch.log(K).to(dtype)  # (B,)\n",
    "        # local indices\n",
    "        # convert k_target to position in grid\n",
    "        # pos = (k_target + b)/dk\n",
    "        pos = (k_target + b) / dk\n",
    "        idx1 = torch.clamp(pos.floor().long(), 0, N-2)     # left index\n",
    "        idx2 = idx1 + 1\n",
    "\n",
    "        k0 = k_grid[idx1]  # (B,)\n",
    "        k1 = k_grid[idx2]\n",
    "        c0 = calls_grid.gather(1, idx1.view(-1,1)).squeeze(1)\n",
    "        c1 = calls_grid.gather(1, idx2.view(-1,1)).squeeze(1)\n",
    "        w1 = (k_target - k0) / (k1 - k0 + 1e-16)\n",
    "        price = c0 + (c1 - c0) * w1  # (B,)\n",
    "\n",
    "        # Optionnel: clamp léger pour stabilité numérique, mais on évite de couper négatifs microscopiques\n",
    "        return price\n",
    "\n",
    "\n",
    "# =============\n",
    "# Neural network\n",
    "# =============\n",
    "\n",
    "class HestonCalibrator(nn.Module):\n",
    "    \"\"\"\n",
    "    NN -> Heston params, then pricing layer -> call price.\n",
    "    Inputs per sample: [r, T, S0, K, iv_bs]  # évite C_mkt en entrée\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden=128, N=4096, eta=0.25, alpha=1.5):\n",
    "        super().__init__()\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(5, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 6)  # raw outputs: kappa, theta, sigma, rho, v0, q_shift\n",
    "        )\n",
    "        self.pricer = HestonPricingLayer(N=N, eta=eta, alpha=alpha)\n",
    "\n",
    "        # On inclut q comme variable exogène? Ici on suppose q=0\n",
    "        # Si tu veux apprendre un petit décalage de dividende effectif sur les données, q_shift sert à ça.\n",
    "        # Sinon mets-le à zéro lors du forward.\n",
    "\n",
    "    def forward(self, x, C_mkt=None):\n",
    "        \"\"\"\n",
    "        x: tensor (B,5) with columns [r, T, S0, K, iv_bs]\n",
    "        C_mkt: optional (B,) market call price (only needed for loss)\n",
    "        Returns:\n",
    "          params dict and C_pred\n",
    "        \"\"\"\n",
    "        r, T, S0, K, iv = x.unbind(dim=1)  # each (B,)\n",
    "        raw = self.feature_net(x)\n",
    "\n",
    "        raw_kappa, raw_theta, raw_sigma, raw_rho, raw_v0, raw_qshift = raw.unbind(dim=1)\n",
    "\n",
    "        # domain constraints\n",
    "        softplus = F.softplus\n",
    "        kappa = softplus(raw_kappa) + 1e-6\n",
    "        theta = softplus(raw_theta) + 1e-8\n",
    "        sigma = softplus(raw_sigma) + 1e-8\n",
    "        rho   = 0.999 * torch.tanh(raw_rho)\n",
    "        v0    = softplus(raw_v0) + 1e-10\n",
    "        q     = torch.zeros_like(r) + 0.0 + 0.0*raw_qshift  # fixe q=0 ; ou utilise petite correction: 1e-3*tanh(raw_qshift)\n",
    "\n",
    "        C_pred = self.pricer(S0, K, T, r, q, kappa, theta, sigma, rho, v0)\n",
    "\n",
    "        out = {\n",
    "            \"kappa\": kappa, \"theta\": theta, \"sigma\": sigma, \"rho\": rho, \"v0\": v0,\n",
    "            \"C_pred\": C_pred\n",
    "        }\n",
    "        if C_mkt is None:\n",
    "            return out\n",
    "        loss = F.mse_loss(C_pred, C_mkt)\n",
    "        out[\"loss\"] = loss\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========\n",
    "# Training\n",
    "# =========\n",
    "\n",
    "\n",
    "def bs_vega(S0, K, T, r, q, vol):\n",
    "    # vega = ∂C/∂vol sous Black–Scholes (approx pour pondérer la perte)\n",
    "    from math import erf, sqrt, log, exp\n",
    "    def ncdf_pdf(x):\n",
    "        # retourne (phi, Phi)\n",
    "        from math import pi\n",
    "        Phi = 0.5*(1.0 + erf(x/sqrt(2.0)))\n",
    "        phi = (1.0/sqrt(2.0*pi))*torch.exp(-0.5*torch.as_tensor(x)**2)\n",
    "        return phi, Phi\n",
    "    S0 = torch.as_tensor(S0); K = torch.as_tensor(K); T = torch.as_tensor(T)\n",
    "    r = torch.as_tensor(r); q = torch.as_tensor(q); vol = torch.as_tensor(vol)\n",
    "    eps = 1e-12\n",
    "    d1 = (torch.log(S0/K) + (r - q + 0.5*vol*vol)*T) / (vol*torch.sqrt(T+eps)+eps)\n",
    "    phi, _ = ncdf_pdf(d1)\n",
    "    return S0*torch.exp(-q*T)*torch.as_tensor(phi)*torch.sqrt(T+eps)\n",
    "\n",
    "def weighted_mse(C_pred, C_mkt, vega, eps=1e-8):\n",
    "    w = 1.0/(vega.abs() + eps)  # plus de poids là où Vega est petit\n",
    "    diff = (C_pred - C_mkt)\n",
    "    return torch.mean((w*diff)**2)\n",
    "\n",
    "def feller_penalty(kappa, theta, sigma):\n",
    "    # Feller: 2*kappa*theta >= sigma^2 ; pénalise les violations\n",
    "    viol = torch.relu(sigma*sigma - 2.0*kappa*theta)\n",
    "    return viol\n",
    "\n",
    "\n",
    "def train_step(model, optimizer, batch_x, batch_Cmkt, lambda_feller=1e-3, lambda_ridge=1e-6):\n",
    "    \"\"\"\n",
    "    batch_x: (B,5) = [r, T, S0, K, iv_bs]\n",
    "    batch_Cmkt: (B,)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(batch_x, C_mkt=batch_Cmkt)\n",
    "    C_pred = out[\"C_pred\"]\n",
    "\n",
    "    r, T, S0, K, iv = batch_x.unbind(dim=1)\n",
    "    vega = bs_vega(S0, K, T, r, torch.zeros_like(r), iv)  # q≈0 pour le poids\n",
    "\n",
    "    mse = weighted_mse(C_pred, batch_Cmkt, vega)\n",
    "\n",
    "    # pénalités sur paramètres\n",
    "    pen_feller = feller_penalty(out[\"kappa\"], out[\"theta\"], out[\"sigma\"]).mean()\n",
    "    pen_ridge  = (out[\"kappa\"].mean() + out[\"theta\"].mean() + out[\"sigma\"].mean()\n",
    "                  + out[\"v0\"].mean() + out[\"rho\"].pow(2).mean())  # léger l2\n",
    "\n",
    "    loss = mse + lambda_feller*pen_feller + lambda_ridge*pen_ridge\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss.detach(),\n",
    "        \"mse\": mse.detach(),\n",
    "        \"feller\": pen_feller.detach()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# =================\n",
    "# Example dummy use\n",
    "# =================\n",
    "if __name__ == \"__main__\":\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    # Fake batch (remplace par tes données réelles)\n",
    "    B = 32\n",
    "    r  = torch.full((B,), 0.01)\n",
    "    T  = torch.linspace(0.1, 2.0, B)\n",
    "    S0 = torch.full((B,), 100.0)\n",
    "    K  = torch.linspace(60.0, 140.0, B)\n",
    "    iv = torch.full((B,), 0.2)\n",
    "\n",
    "    # Prix marché factices: par exemple Black-Scholes comme cible\n",
    "    # En pratique, mets tes vrais prix (mid) ici\n",
    "    from math import erf, sqrt, log, exp\n",
    "    def ncdf(x): return 0.5*(1.0 + erf(x/sqrt(2.0)))\n",
    "    def bs_call(S0,K,T,r,q,vol):\n",
    "        if T<=0 or vol<=0: return max(0.0, S0 - K*exp(-r*T))\n",
    "        d1 = (log(S0/K)+(r - q + 0.5*vol*vol)*T)/(vol*sqrt(T))\n",
    "        d2 = d1 - vol*sqrt(T)\n",
    "        return S0*exp(-q*T)*ncdf(d1) - K*exp(-r*T)*ncdf(d2)\n",
    "\n",
    "    C_mkt = torch.tensor([bs_call(S0[i].item(), K[i].item(), T[i].item(), r[i].item(), 0.0, iv[i].item())\n",
    "                          for i in range(B)], dtype=torch.float64)\n",
    "\n",
    "    X = torch.stack([r, T, S0, K, iv], dim=1)\n",
    "\n",
    "    model = HestonCalibrator(hidden=128, N=2048, eta=0.20, alpha=1.5)  # N=2048 pour accélérer l’exemple\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(5):  # démo courte\n",
    "        stats = train_step(model, opt, X, C_mkt)\n",
    "        print(f\"epoch {epoch} | loss {stats['loss'].item():.6e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "        print(\"kappa mean:\", preds[\"kappa\"].mean().item(),\n",
    "              \"theta mean:\", preds[\"theta\"].mean().item(),\n",
    "              \"sigma mean:\", preds[\"sigma\"].mean().item(),\n",
    "              \"rho mean:\", preds[\"rho\"].mean().item(),\n",
    "              \"v0 mean:\", preds[\"v0\"].mean().item())\n"
   ],
   "id": "d4d32740d6ac6506",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 262\u001B[39m\n\u001B[32m    259\u001B[39m opt = torch.optim.Adam(model.parameters(), lr=\u001B[32m1e-3\u001B[39m)\n\u001B[32m    261\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m5\u001B[39m):  \u001B[38;5;66;03m# démo courte\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m     stats = \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC_mkt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mepoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstats[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m].item()\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.6e\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    265\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 223\u001B[39m, in \u001B[36mtrain_step\u001B[39m\u001B[34m(model, optimizer, batch_x, batch_Cmkt)\u001B[39m\n\u001B[32m    221\u001B[39m model.train()\n\u001B[32m    222\u001B[39m optimizer.zero_grad()\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m out = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC_mkt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_Cmkt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    224\u001B[39m out[\u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m].backward()\n\u001B[32m    225\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/myQuantGuild/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/myQuantGuild/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 199\u001B[39m, in \u001B[36mHestonCalibrator.forward\u001B[39m\u001B[34m(self, x, C_mkt)\u001B[39m\n\u001B[32m    196\u001B[39m v0    = softplus(raw_v0) + \u001B[32m1e-10\u001B[39m\n\u001B[32m    197\u001B[39m q     = torch.zeros_like(r) + \u001B[32m0.0\u001B[39m + \u001B[32m0.0\u001B[39m*raw_qshift  \u001B[38;5;66;03m# fixe q=0 ; ou utilise petite correction: 1e-3*tanh(raw_qshift)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m C_pred = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpricer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mS0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkappa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrho\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    201\u001B[39m out = {\n\u001B[32m    202\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mkappa\u001B[39m\u001B[33m\"\u001B[39m: kappa, \u001B[33m\"\u001B[39m\u001B[33mtheta\u001B[39m\u001B[33m\"\u001B[39m: theta, \u001B[33m\"\u001B[39m\u001B[33msigma\u001B[39m\u001B[33m\"\u001B[39m: sigma, \u001B[33m\"\u001B[39m\u001B[33mrho\u001B[39m\u001B[33m\"\u001B[39m: rho, \u001B[33m\"\u001B[39m\u001B[33mv0\u001B[39m\u001B[33m\"\u001B[39m: v0,\n\u001B[32m    203\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mC_pred\u001B[39m\u001B[33m\"\u001B[39m: C_pred\n\u001B[32m    204\u001B[39m }\n\u001B[32m    205\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m C_mkt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/myQuantGuild/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1735\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda/envs/myQuantGuild/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1743\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1745\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1746\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1747\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1749\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1750\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mHestonPricingLayer.forward\u001B[39m\u001B[34m(self, S0, K, T, r, q, kappa, theta, sigma, rho, v0)\u001B[39m\n\u001B[32m    117\u001B[39m psi = torch.exp(-r[:,\u001B[38;5;28;01mNone\u001B[39;00m]*T[:,\u001B[38;5;28;01mNone\u001B[39;00m]) * phi_shift / denom  \u001B[38;5;66;03m# (B,N) complex\u001B[39;00m\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m# FFT coupling\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m dk = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m(device=device)\n\u001B[32m    121\u001B[39m b  = \u001B[38;5;28mself\u001B[39m.b.to(device=device)\n\u001B[32m    122\u001B[39m x  = psi * torch.exp(\u001B[32m1\u001B[39mj * b * v) * w  \u001B[38;5;66;03m# (B,N) complex\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'float' object has no attribute 'to'"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
